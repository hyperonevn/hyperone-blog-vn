---
title: 'Switch Transformers – Khi mô hình bắt đầu học cách tiết kiệm sức mạnh não bộ'
date: '2021-01-13'
tags: ['AI', 'CÔNG-NGHỆ']
draft: false
summary: 'Đầu năm 2021, giới NLP xôn xao với một câu hỏi kỳ lạ: “Liệu có phải mọi neuron trong một mô hình khổ'
---

# Switch Transformers – Khi mô hình bắt đầu học cách tiết kiệm sức mạnh não bộ

Đầu năm 2021, giới NLP xôn xao với một câu hỏi kỳ lạ: “Liệu có phải mọi neuron trong một mô hình khổng lồ đều cần hoạt động cho mọi câu hỏi?” Các mô hình như T5 hay GPT-3 đi theo triết lý càng lớn càng tốt, nhưng cái giá phải trả là chi phí tính toán đắt đỏ, độ trễ cao và tác động môi trường không hề nhỏ. Google giới thiệu Switch Transformers như một lời nhắc: trí tuệ cũng cần biết tiết kiệm năng lượng.

Switch Transformers áp dụng tư tưởng Mixture-of-Experts, nhưng ở quy mô rất lớn: hàng triệu “chuyên gia con” được huấn luyện để xử lý từng phần nhiệm vụ, và với mỗi token, chỉ có một số ít chuyên gia được kích hoạt. Giống như trong một công ty, không phải cả đội phải tham gia vào từng việc nhỏ. Điều này giúp mô hình mở rộng kích thước tham số khổng lồ nhưng chi phí suy luận lại không phình theo tỷ lệ tương ứng.

Năm 2021, câu chuyện về Switch Transformers gây phấn khích: AI có thể khôn ngoan hơn trong việc dùng tài nguyên. Tuy nhiên, điều đó đi kèm thách thức: làm sao để router chọn đúng chuyên gia thay vì biến mọi thứ thành hỗn loạn? Switch cho thấy kỷ nguyên “lấy lực đè người” đang chuyển sang “tối ưu đầu óc trước khi tăng cơ bắp”.